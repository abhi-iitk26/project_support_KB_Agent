# Support KB Agent

## MCP + LangGraph + RAG Demo

---

## 1. Project Overview

This project implements a **Support Knowledge Base (KB) Agent** that answers user questions by retrieving information from a document collection (PDFs/text files) and generating grounded responses using a large language model.

The system demonstrates how to combine:

- **RAG** (Retrieval-Augmented Generation)
- **LangGraph** for orchestration
- **MCP** (Model Context Protocol) for tool exposure
- **Groq LLM** for fast inference

The agent retrieves relevant document chunks, reasons over them, and produces answers with citations.

---

## 2. Features

- ✅ PDF / text document ingestion
- ✅ Semantic chunking and embedding
- ✅ Vector database (ChromaDB)
- ✅ Context-aware question answering
- ✅ Source citation with page numbers
- ✅ LangGraph-based multi-step workflow
- ✅ MCP-based tool exposure

---

## 3. Architecture

### LangGraph Workflow

```
User Question
      ↓
┌─────────────────┐
│    retrieve     │  ← Fetch relevant docs from ChromaDB
└────────┬────────┘
         ↓
┌─────────────────┐
│     draft       │  ← Generate answer using LLM + context
└────────┬────────┘
         ↓
┌─────────────────┐
│      cite       │  ← Extract source citations
└────────┬────────┘
         ↓
┌─────────────────┐
│     final       │  ← Combine answer + sources
└─────────────────┘
```

### Visual Graph (Generated by LangGraph)

<p align="center">
  <img src="support_kb_architecture.png" alt="LangGraph Workflow" width="200"/>
</p>

The graph shows the sequential flow:
- **`__start__`** → Entry point
- **`retrieve`** → Vector search in ChromaDB
- **`draft`** → LLM generates answer with citations
- **`cite`** → Extract source metadata
- **`final`** → Format final response
- **`__end__`** → Output

---

## 4. Project Structure

```
Agentic_workflow/
│
├── app.py                    # Main entry point
├── draw_graph.py             # Generate architecture diagram
├── requirements.txt          # Dependencies
├── README.md
│
├── data/
│   ├── docs/                 # Source documents
│   ├── pdfs/                 # PDF files for ingestion
│   ├── raw_json/             # Extracted JSON from PDFs
│   └── semantic_chunks.json  # Chunked data
│
├── chroma_db/                # Persistent vector store
│
├── rag/
│   ├── pdf_to_raw_json.py    # PDF extraction (Unstructured)
│   ├── chunking.py           # Semantic chunking
│   ├── chroma_indexer.py     # Embedding + indexing
│   ├── vector_store.py       # ChromaDB connection
│   └── retriever.py          # Retrieval function
│
├── graph/
│   └── agent_graph.py        # LangGraph workflow
│
├── tools/
│   └── retrieval_tool.py     # search_kb tool
│
└── mcp_server/
    ├── server.py             # FastMCP server
    └── tools.py              # Tool definitions
```

---

## 5. Setup Instructions

### Step 1 – Create Virtual Environment

```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
```

### Step 2 – Install Dependencies

```powershell
pip install -r requirements.txt
```

### Step 3 – Set Groq API Key

```powershell
$env:GROQ_API_KEY="your_groq_api_key_here"
```

Or permanently:

```powershell
setx GROQ_API_KEY "your_groq_api_key_here"
```

---

## 6. Build the Knowledge Base

### Step 1 – Extract PDFs to JSON

```powershell
python rag/pdf_to_raw_json.py
```

### Step 2 – Create Semantic Chunks

```powershell
python rag/chunking.py
```

### Step 3 – Index into ChromaDB

```powershell
python rag/chroma_indexer.py
```

---

## 7. Run the Agent

```powershell
python app.py
```

---

## 8. Demo

### Sample Query & Response

```
Ask Support KB: tell me about supervised learning

Supervised learning is a branch of machine learning that concentrates on 
learning patterns through connecting the relationship between variables and 
known outcomes and working with labeled datasets [3]. It involves feeding 
the machine sample data with various features (represented as "X") and the 
correct value output of the data (represented as "y") [3]. The algorithm 
then deciphers patterns that exist in the data and creates a model that 
can reproduce the same underlying rules with new data [2].

In supervised learning, the machine is trained on a labeled dataset, where 
the output and feature values are known [3]. The algorithm works by analyzing 
the relationship between the input features and the output values, and creates 
a model that can make predictions on new, unseen data [3]. For example, to 
predict the market rate for the purchase of a used car, a supervised algorithm 
can formulate predictions by analyzing the relationship between car attributes 
(including the year of make, car brand, mileage, etc.) and the selling price 
of other cars sold based on historical data [3].

Supervised learning is a widely used technique in machine learning, and has 
many applications in areas such as regression analysis, decision trees, 
k-nearest neighbors, neural networks, and support vector machines [2].

The process of supervised learning involves several steps, including data 
preparation, model training, and model testing [1]. After the model is 
prepared, it can be applied to new data and tested for accuracy [2]. If 
the model performs well on the test data, it can be used to make predictions 
on new, unseen data [1].

References:
[1] Context: introduction to machine learning
[2] Context: machine learning algorithms
[3] Context: supervised learning explanation

Sources:
[1] Machine Learning For Absolute Beginners.pdf pages 11,12,13
[2] Machine Learning For Absolute Beginners.pdf pages 16,17,18
[3] Machine Learning For Absolute Beginners.pdf pages 15,16
[4] Machine Learning For Absolute Beginners.pdf pages 9,10
```

### Agent Flow in Action

The LangGraph workflow executes 4 nodes sequentially:

| Step | Node | Action |
|------|------|--------|
| 1 | `retrieve` | Query ChromaDB → Return top-4 relevant chunks |
| 2 | `draft` | Send context + question to Groq LLM → Generate answer |
| 3 | `cite` | Extract source file + page numbers from metadata |
| 4 | `final` | Combine draft answer + formatted citations |

---

## 9. MCP Server

Start the MCP server to expose retrieval as a tool:

```powershell
python mcp_server/server.py
```

The server runs at `http://127.0.0.1:3333` and exposes:

- `search(query: str)` → Returns relevant document chunks

---

## 10. Design Choices

### PDF Extraction

- **Unstructured** library with `hi_res` strategy for layout detection
- Extracts `NarrativeText` and `ListItem` elements only (ignores images/tables)
- Preserves chapter/section titles and page numbers as metadata
- Outputs structured JSON per PDF for downstream processing

### Chunking Strategy

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| Min tokens | 400 | Ensures enough context per chunk |
| Max tokens | 500 | Fits within embedding model limits |
| Overlap | 25% | Preserves context across boundaries |
| Scope | Per-PDF | Never mixes content from different sources |

**Why semantic chunking?**
- Fixed-size splits break sentences mid-thought
- Semantic chunking respects paragraph boundaries
- Overlap ensures no information loss at edges

### Retrieval

| Component | Choice | Why |
|-----------|--------|-----|
| Embedding | `all-MiniLM-L6-v2` | Fast, lightweight (80MB), good for English |
| Vector DB | ChromaDB | Simple, persistent, no external deps |
| Top-K | 4 | Balances context size vs relevance |
| Search | Cosine similarity | Standard for dense retrieval |

**Retrieval flow:**
```
Query → Embed → ChromaDB search → Top-4 chunks + metadata
```

### LLM

- **Groq** with `llama-3.3-70b-versatile`
- Fast inference (< 1s response time)
- Good instruction following for RAG tasks

### Prompt Design

- Explicit instruction to use **only** provided context
- Citation format enforced (`[1]`, `[2]`, etc.)
- Grounded answers reduce hallucination

---

## 11. Document Sources

| Document | Description |
|----------|-------------|
| Machine Learning For Absolute Beginners.pdf | ML fundamentals |
| Artificial Intelligence Risk Management.pdf | AI risk frameworks |
| ETHICS GUIDELINES FOR TRUSTWORTHY AI.pdf | AI ethics guidelines |
| RAG.pdf | RAG architecture concepts |

---

## 12. Sample Queries to Try

```
Ask Support KB: What is machine learning?
Ask Support KB: Explain neural networks
Ask Support KB: What are the risks of AI?
Ask Support KB: How does RAG work?
Ask Support KB: What is ethical AI?
```

Type `quit` to exit.

---

